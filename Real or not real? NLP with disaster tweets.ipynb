{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Challenge: Disaster, real or not real?\n",
    "-------------------------------------------------------------------\n",
    "### Context:\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. Our goal is to be able to build a supervised learning model that allows us to predict whether a tweet about disaster is real or not real.\n",
    "\n",
    "### Content:\n",
    "This dataset contains 7613 tweets from Twitter with labels that shows either the tweet is about a real disaster (1), or not a real disaster (0).\n",
    "\n",
    "In order to process the tweet, we'll use natural language processing techniques to prepare text data. Now let's start!\n",
    "\n",
    "### I. Predict disaster tweets with Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import itertools\n",
    "flatten = itertools.chain.from_iterable\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset:\n",
    "df = pd.read_csv('train.csv', usecols=['text', 'target'])\n",
    "\n",
    "# Print out the shape of the dataset:\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
       "5  #RockyFire Update => California Hwy. 20 closed...       1\n",
       "6  #flood #disaster Heavy rain causes flash flood...       1\n",
       "7  I'm on top of the hill and I can see a fire in...       1\n",
       "8  There's an emergency evacuation happening now ...       1\n",
       "9  I'm afraid that the tornado is coming to our a...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 rows of the train dataset:\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 2 columns):\n",
      "text      7613 non-null object\n",
      "target    7613 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for text cleaning:\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text) \n",
    "    text = re.sub(r'http\\S+',r'',text) \n",
    "    text = re.sub(r'[?|!|\\'|\"|#]',r'',text) \n",
    "    text = re.sub(r'[.|,|)|(|)|\\|/]',r' ',text) \n",
    "    text = text.lower() \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[()[\\]{}\\''',.``?:;!&^]','',text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('#','',text)\n",
    "    text = re.sub('û*','',text)\n",
    "    text = re.sub('ûó*','',text)\n",
    "    text = re.sub('ò*','',text)\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    text = text.strip() \n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply text_cleaner function:\n",
    "df['parsed_text'] = df['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>parsed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                         parsed_text  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  people receive wildfires evacuation orders in ...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of df to later use for tf-idf:\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the document: \n",
    "nlp = spacy.load('en')\n",
    "df['parsed_text'] = df['parsed_text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function tocreate bag of words:\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " 's',\n",
       " 'like',\n",
       " 'fire',\n",
       " 'be',\n",
       " 'amp',\n",
       " 'new',\n",
       " 'go',\n",
       " 'get',\n",
       " 'people',\n",
       " 'news',\n",
       " 'kill',\n",
       " 'burn',\n",
       " 'year',\n",
       " 'video',\n",
       " 'bomb',\n",
       " 'crash',\n",
       " 'disaster',\n",
       " 'emergency',\n",
       " 'come']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the list of the most common words:\n",
    "common_words = bag_of_words(list(flatten(df['parsed_text'])))\n",
    "common_words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences['parsed_text']\n",
    "    df['text_source'] = sentences['target']\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n",
      "Processing row 2000\n",
      "Processing row 2050\n",
      "Processing row 2100\n",
      "Processing row 2150\n",
      "Processing row 2200\n",
      "Processing row 2250\n",
      "Processing row 2300\n",
      "Processing row 2350\n",
      "Processing row 2400\n",
      "Processing row 2450\n",
      "Processing row 2500\n",
      "Processing row 2550\n",
      "Processing row 2600\n",
      "Processing row 2650\n",
      "Processing row 2700\n",
      "Processing row 2750\n",
      "Processing row 2800\n",
      "Processing row 2850\n",
      "Processing row 2900\n",
      "Processing row 2950\n",
      "Processing row 3000\n",
      "Processing row 3050\n",
      "Processing row 3100\n",
      "Processing row 3150\n",
      "Processing row 3200\n",
      "Processing row 3250\n",
      "Processing row 3300\n",
      "Processing row 3350\n",
      "Processing row 3400\n",
      "Processing row 3450\n",
      "Processing row 3500\n",
      "Processing row 3550\n",
      "Processing row 3600\n",
      "Processing row 3650\n",
      "Processing row 3700\n",
      "Processing row 3750\n",
      "Processing row 3800\n",
      "Processing row 3850\n",
      "Processing row 3900\n",
      "Processing row 3950\n",
      "Processing row 4000\n",
      "Processing row 4050\n",
      "Processing row 4100\n",
      "Processing row 4150\n",
      "Processing row 4200\n",
      "Processing row 4250\n",
      "Processing row 4300\n",
      "Processing row 4350\n",
      "Processing row 4400\n",
      "Processing row 4450\n",
      "Processing row 4500\n",
      "Processing row 4550\n",
      "Processing row 4600\n",
      "Processing row 4650\n",
      "Processing row 4700\n",
      "Processing row 4750\n",
      "Processing row 4800\n",
      "Processing row 4850\n",
      "Processing row 4900\n",
      "Processing row 4950\n",
      "Processing row 5000\n",
      "Processing row 5050\n",
      "Processing row 5100\n",
      "Processing row 5150\n",
      "Processing row 5200\n",
      "Processing row 5250\n",
      "Processing row 5300\n",
      "Processing row 5350\n",
      "Processing row 5400\n",
      "Processing row 5450\n",
      "Processing row 5500\n",
      "Processing row 5550\n",
      "Processing row 5600\n",
      "Processing row 5650\n",
      "Processing row 5700\n",
      "Processing row 5750\n",
      "Processing row 5800\n",
      "Processing row 5850\n",
      "Processing row 5900\n",
      "Processing row 5950\n",
      "Processing row 6000\n",
      "Processing row 6050\n",
      "Processing row 6100\n",
      "Processing row 6150\n",
      "Processing row 6200\n",
      "Processing row 6250\n",
      "Processing row 6300\n",
      "Processing row 6350\n",
      "Processing row 6400\n",
      "Processing row 6450\n",
      "Processing row 6500\n",
      "Processing row 6550\n",
      "Processing row 6600\n",
      "Processing row 6650\n",
      "Processing row 6700\n",
      "Processing row 6750\n",
      "Processing row 6800\n",
      "Processing row 6850\n",
      "Processing row 6900\n",
      "Processing row 6950\n",
      "Processing row 7000\n",
      "Processing row 7050\n",
      "Processing row 7100\n",
      "Processing row 7150\n",
      "Processing row 7200\n",
      "Processing row 7250\n",
      "Processing row 7300\n",
      "Processing row 7350\n",
      "Processing row 7400\n",
      "Processing row 7450\n",
      "Processing row 7500\n",
      "Processing row 7550\n",
      "Processing row 7600\n"
     ]
    }
   ],
   "source": [
    "# Apply bow_features function:\n",
    "word_counts = bow_features(df[['parsed_text','target']], common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>not</th>\n",
       "      <th>s</th>\n",
       "      <th>like</th>\n",
       "      <th>fire</th>\n",
       "      <th>be</th>\n",
       "      <th>amp</th>\n",
       "      <th>new</th>\n",
       "      <th>go</th>\n",
       "      <th>get</th>\n",
       "      <th>people</th>\n",
       "      <th>...</th>\n",
       "      <th>lmfao</th>\n",
       "      <th>newyork</th>\n",
       "      <th>credit</th>\n",
       "      <th>tbt</th>\n",
       "      <th>demand</th>\n",
       "      <th>muslim</th>\n",
       "      <th>raid</th>\n",
       "      <th>palestine</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(forest, fire, near, la, ronge, sask, canada)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(people, receive, wildfires, evacuation, order...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   not  s  like  fire  be  amp  new  go  get  people  ...  lmfao  newyork  \\\n",
       "0    0  0     0     0   0    0    0   0    0       0  ...      0        0   \n",
       "1    0  0     0     1   0    0    0   0    0       0  ...      0        0   \n",
       "2    0  0     0     0   0    0    0   0    0       0  ...      0        0   \n",
       "3    0  0     0     0   0    0    0   0    0       1  ...      0        0   \n",
       "4    0  0     0     0   0    0    0   0    1       0  ...      0        0   \n",
       "\n",
       "   credit  tbt  demand  muslim  raid  palestine  \\\n",
       "0       0    0       0       0     0          0   \n",
       "1       0    0       0       0     0          0   \n",
       "2       0    0       0       0     0          0   \n",
       "3       0    0       0       0     0          0   \n",
       "4       0    0       0       0     0          0   \n",
       "\n",
       "                                       text_sentence  text_source  \n",
       "0  (our, deeds, are, the, reason, of, this, earth...            1  \n",
       "1      (forest, fire, near, la, ronge, sask, canada)            1  \n",
       "2  (all, residents, asked, to, shelter, in, place...            1  \n",
       "3  (people, receive, wildfires, evacuation, order...            1  \n",
       "4  (just, got, sent, this, photo, from, ruby, ala...            1  \n",
       "\n",
       "[5 rows x 2002 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Supervised learning models (using bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model:\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.70209974 0.44488189 0.47900262 0.42838371 0.52299606 0.59658344\n",
      " 0.59001314 0.5781866  0.69382392 0.61760841]\n",
      "Mean score:  0.5653579521350895\n"
     ]
    }
   ],
   "source": [
    "rfc_scores = cross_val_score(rfc, X, Y, cv=10)\n",
    "print(\"Cross-validation scores: \",rfc_scores)\n",
    "print(\"Mean score: \",rfc_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.71391076 0.5656168  0.48950131 0.44283837 0.51642576 0.60183968\n",
      " 0.57555848 0.57950066 0.66491459 0.71222076]\n",
      "Mean score:  0.5862327163112495\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model:\n",
    "lr = LogisticRegression(penalty='l1',multi_class='auto',solver='liblinear')\n",
    "\n",
    "lr_scores = cross_val_score(lr, X, Y, cv=10)\n",
    "print(\"Cross-validation scores: \",lr_scores)\n",
    "print(\"Mean score: \",lr_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross_validation scores:  [0.65354331 0.64829396 0.4855643  0.57161629 0.46780552 0.58344284\n",
      " 0.58607096 0.63600526 0.65571616 0.73455979]\n",
      "Mean score:  0.6022618394776869\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting model:\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "clf_scores = cross_val_score(clf, X, Y, cv=10)\n",
    "print(\"Cross_validation scores: \",clf_scores)\n",
    "print(\"Mean score: \",clf_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Predict disaster tweets with Tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define our TfidfVectorizer:\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least 3 times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4032\n"
     ]
    }
   ],
   "source": [
    "# Applying the vectorizer to our dataset\n",
    "tfidf = vectorizer.fit_transform(df1['parsed_text'])\n",
    "print(\"Number of features: %d\" % tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4032)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 4032) (7613,)\n"
     ]
    }
   ],
   "source": [
    "# Define explanatory and target variable:\n",
    "X = tfidf\n",
    "Y = df1['target']\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning models (using tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.71128609 0.54855643 0.47900262 0.46911958 0.54270696 0.58738502\n",
      " 0.61366623 0.61498029 0.64257556 0.71353482]\n",
      "Mean score:  0.5922813606906233\n"
     ]
    }
   ],
   "source": [
    "# Random forest model:\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X, Y, cv=10)\n",
    "print(\"Cross-validation scores: \",rfc_scores)\n",
    "print(\"Mean score: \",rfc_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.7335958  0.65091864 0.59973753 0.60709593 0.63731932 0.66491459\n",
      " 0.68068331 0.6544021  0.73587385 0.76609724]\n",
      "Mean score:  0.673063830227529\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model:\n",
    "lr = LogisticRegression(penalty='l2',multi_class='auto',solver='saga')\n",
    "\n",
    "lr_scores = cross_val_score(lr, X, Y, cv=10)\n",
    "print(\"Cross-validation scores: \",lr_scores)\n",
    "print(\"Mean score: \",lr_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross_validation scores:  [0.65485564 0.6351706  0.49343832 0.62417871 0.51116951 0.59921156\n",
      " 0.59395532 0.61760841 0.63206307 0.73587385]\n",
      "Mean score:  0.6097525013709686\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting model:\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "clf_scores = cross_val_score(clf, X, Y, cv=10)\n",
    "print(\"Cross_validation scores: \",clf_scores)\n",
    "print(\"Mean score: \",clf_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
