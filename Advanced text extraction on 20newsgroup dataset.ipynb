{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20newsgroup dataset:\n",
    "df = sklearn.datasets.fetch_20newsgroups(subset='all', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the dataset into a dataframe\n",
    "df_news = pd.DataFrame(df).T\n",
    "\n",
    "# Add column name\n",
    "df_news.columns = ['text', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18841</td>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18842</td>\n",
       "      <td>From: rdell@cbnewsf.cb.att.com (richard.b.dell...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18843</td>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18844</td>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18845</td>\n",
       "      <td>From: chriss@netcom.com (Chris Silvester)\\nSub...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text target\n",
       "0      From: Mamatha Devineni Ratnam <mr47+@andrew.cm...     10\n",
       "1      From: mblawson@midway.ecn.uoknor.edu (Matthew ...      3\n",
       "2      From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     17\n",
       "3      From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...      3\n",
       "4      From: Alexander Samuel McDiarmid <am2o+@andrew...      4\n",
       "...                                                  ...    ...\n",
       "18841  From: jim.zisfein@factory.com (Jim Zisfein) \\n...     13\n",
       "18842  From: rdell@cbnewsf.cb.att.com (richard.b.dell...     12\n",
       "18843  From: westes@netcom.com (Will Estes)\\nSubject:...      3\n",
       "18844  From: steve@hcrlgw (Steven Collins)\\nSubject: ...      1\n",
       "18845  From: chriss@netcom.com (Chris Silvester)\\nSub...      7\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text:\n",
    "\n",
    "def text_cleaner(text):\n",
    "# Newsgroup posters have fancy ASCII signatures (ahh, the late 80s/early 90s).\n",
    "# Obliterate all of the repetitive punctuation that typifies them.\n",
    "    text = re.sub(r'[-+=~_/\\\\|\\^%:\\.]{2,}',' ',text)\n",
    "# Remove From lines, and the word \"Subject: \" when it is at the beginning of a line\n",
    "    text = re.sub(r'^From.*','',text)\n",
    "    text = re.sub(r'Subject:\\s*',' ',text)\n",
    "# Remove \"\\n\" from text:\n",
    "    text = re.sub(r'\\\\n',' ',text)\n",
    "# Remove \"words\" that are nothing but spaces and digits, possibly with embedded parenthesis and dashes \n",
    "    text = re.sub(r'\\s*[\\(\\)0-9]+\\s*', '', text,flags=re.M)\n",
    "# (This was a poorly-written rule that ended up eating strings of digits that provided useful information/context)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['cleaned_text'] = df_news['text'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "df_tfidf=vectorizer.fit_transform(df_news['cleaned_text'])\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics.\n",
    "ntopics=20\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=[x for x in chosenlist]\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "df_lsa = lsa.fit_transform(df_tfidf)\n",
    "\n",
    "components_lsa = word_topic(df_tfidf, df_lsa, terms)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "df_lda = lda.fit_transform(df_tfidf) \n",
    "\n",
    "components_lda = word_topic(df_tfidf, df_lda, terms)\n",
    "\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to sto p even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "df_nmf = nmf.fit_transform(df_tfidf) \n",
    "\n",
    "components_nmf = word_topic(df_tfidf, df_nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "                   LSA                LDA          NNMF\n",
      "0           edu 253.12           edu 8.23      car 3.71\n",
      "0         lines 175.56           com 5.97     like 1.98\n",
      "0  organization 174.23         lines 5.69     just 1.96\n",
      "0           com 163.86  organization 5.68      edu 1.94\n",
      "0       article 158.51       article 5.41      don 1.91\n",
      "0    university 155.96    university 4.81     bike 1.78\n",
      "0           don 143.14       posting 4.51      com 1.72\n",
      "0        posting 141.3           don 4.49     good 1.69\n",
      "0          just 138.47          host 4.35  article 1.68\n",
      "0          like 137.68          know 4.22     know 1.56\n",
      "Topic 1:\n",
      "                LSA                LDA             NNMF\n",
      "1         god 77.71           edu 6.13         god 12.6\n",
      "1      people 40.68         lines 4.33       jesus 5.02\n",
      "1       jesus 35.34  organization 4.18       bible 3.24\n",
      "1   christian 25.28           com 3.78      people 3.03\n",
      "1       bible 24.03    university 3.54     believe 2.75\n",
      "1      believe 22.9       posting 3.53   christian 2.75\n",
      "1  christians 21.46       article 3.51      christ 2.65\n",
      "1          say 21.0          host 3.42  christians 2.53\n",
      "1      christ 20.05          nntp 3.32       faith 2.52\n",
      "1      church 19.67          like 2.82         sin 2.27\n",
      "Topic 2:\n",
      "               LSA                LDA               NNMF\n",
      "2        god 63.99            edu 6.4          file 2.71\n",
      "2    windows 43.88  organization 5.04        window 2.71\n",
      "2      jesus 25.66         lines 4.93       program 2.38\n",
      "2        dos 22.12           com 4.49           edu 2.36\n",
      "2       file 18.32    university 4.12      graphics 2.23\n",
      "2      bible 18.31       article 3.97         lines 2.21\n",
      "2      drive 17.39          host 3.74  organization 2.16\n",
      "2  christian 16.34       posting 3.71        thanks 2.13\n",
      "2       card 15.99          know 3.66           use 1.99\n",
      "2         pc 15.74          nntp 3.54            uk 1.98\n",
      "Topic 3:\n",
      "                LSA                LDA             NNMF\n",
      "3         key 40.47           edu 7.29         key 7.27\n",
      "3     clipper 36.16         lines 5.09      clipper 6.5\n",
      "3        chip 31.01  organization 5.01        chip 5.67\n",
      "3  encryption 29.47    university 4.76  encryption 5.55\n",
      "3  government 26.56           com 4.63      escrow 3.54\n",
      "3        keys 20.09       article 4.05        keys 3.54\n",
      "3      escrow 18.33       posting 3.93  government 3.29\n",
      "3         use 17.15          host 3.86   algorithm 2.47\n",
      "3      public 14.57          nntp 3.72     security 2.1\n",
      "3   algorithm 14.24          know 3.61          use 2.1\n",
      "Topic 4:\n",
      "               LSA                LDA               NNMF\n",
      "4     israel 26.31           edu 6.46        israel 4.44\n",
      "4    israeli 17.92         lines 4.97       israeli 2.69\n",
      "4        jews 16.8  organization 4.77          jews 2.11\n",
      "4       drive 13.3    university 4.53          arab 1.64\n",
      "4     people 13.24            com 4.2        jewish 1.42\n",
      "4   armenian 12.41       posting 3.68         arabs 1.28\n",
      "4    turkish 12.26       article 3.64         peace 1.02\n",
      "4       arab 11.61           host 3.5        people 1.01\n",
      "4  armenians 10.97          nntp 3.41            edu 1.0\n",
      "4        gun 10.92          know 3.35  palestinians 0.84\n",
      "Topic 5:\n",
      "                LSA                LDA            NNMF\n",
      "5       drive 43.61           edu 8.78       scsi 9.18\n",
      "5        scsi 37.89         lines 6.47       drive 7.4\n",
      "5          ide 21.2           com 6.35        ide 4.65\n",
      "5        game 15.97  organization 6.14  controller 2.6\n",
      "5  controller 15.39       article 6.06     drives 2.51\n",
      "5        hard 15.23    university 4.92       hard 2.47\n",
      "5      drives 15.02        posting 4.8       disk 2.42\n",
      "5        disk 13.28          host 4.67         mb 1.55\n",
      "5        team 12.88          pitt 4.51        bus 1.54\n",
      "5         bus 11.43          nntp 4.44        mac 1.52\n",
      "Topic 6:\n",
      "             LSA                LDA           NNMF\n",
      "6  windows 45.11           edu 7.14      game 6.42\n",
      "6     game 24.55         lines 5.22      team 5.06\n",
      "6     team 21.36  organization 5.12    hockey 3.67\n",
      "6      dos 18.82           com 4.53      games 3.3\n",
      "6   window 17.68    university 4.52       edu 2.99\n",
      "6     file 17.39       article 4.47      year 2.91\n",
      "6      win 16.62       posting 3.98  baseball 2.64\n",
      "6   hockey 12.92          host 3.82      espn 2.53\n",
      "6  program 12.48          nntp 3.68   players 2.53\n",
      "6     year 12.42           don 3.63       don 2.37\n",
      "Topic 7:\n",
      "                LSA                LDA               NNMF\n",
      "7      israel 24.17           edu 9.58           edu 8.39\n",
      "7         edu 21.42         lines 6.79    university 4.24\n",
      "7  university 19.38  organization 6.48        posting 4.1\n",
      "7        game 16.79           com 6.12          nntp 4.07\n",
      "7     israeli 15.53       armenian 6.0          host 4.04\n",
      "7         key 15.21       article 5.77  organization 3.47\n",
      "7        jews 14.63    university 5.67         lines 3.44\n",
      "7      jewish 12.98       posting 5.61       article 2.86\n",
      "7        team 12.57          host 5.41            cs 2.21\n",
      "7     clipper 12.02     armenians 5.21           apr 2.15\n",
      "Topic 8:\n",
      "              LSA                LDA          NNMF\n",
      "8     space 46.42           edu 5.39    space 6.49\n",
      "8      nasa 37.48         lines 4.15     nasa 5.48\n",
      "8       gov 19.48  organization 4.12      gov 2.71\n",
      "8   shuttle 13.95           com 3.77      edu 2.05\n",
      "8     henry 12.95    university 3.38  shuttle 1.99\n",
      "8   toronto 12.08       article 3.29    henry 1.67\n",
      "8     orbit 12.07       posting 3.23   toronto 1.5\n",
      "8      moon 11.86          host 3.17   launch 1.45\n",
      "8  research 11.31          nntp 3.07    orbit 1.44\n",
      "8    launch 10.74          like 2.68   alaska 1.43\n",
      "Topic 9:\n",
      "              LSA                LDA            NNMF\n",
      "9  armenian 20.17           edu 9.15   armenian 5.58\n",
      "9   turkish 18.88  organization 6.16  armenians 4.67\n",
      "9  armenians 17.6         lines 6.03    turkish 4.66\n",
      "9   armenia 12.75           com 5.88    armenia 3.17\n",
      "9       god 11.21    university 5.62      turks 2.54\n",
      "9       com 10.99        article 5.4   genocide 2.38\n",
      "9      turks 9.93       posting 5.07     soviet 2.17\n",
      "9     turkey 9.75           host 4.9      muslim 2.0\n",
      "9       nntp 8.95          nntp 4.68     turkey 1.94\n",
      "9      world 8.94          know 4.07     people 1.73\n",
      "Topic 10:\n",
      "             LSA               LDA             NNMF\n",
      "10     car 40.66          edu 5.24      people 4.04\n",
      "10     bike 19.6         lines 3.9         gun 3.71\n",
      "10     new 15.88  organization 3.8          fbi 3.1\n",
      "10    sale 14.48           com 3.7  government 2.92\n",
      "10  thanks 12.83   university 3.31        batf 2.79\n",
      "10   price 12.54      article 3.17         edu 2.52\n",
      "10    cars 12.46      posting 3.03         don 2.46\n",
      "10    good 11.55         host 2.96      koresh 2.37\n",
      "10      ve 11.33         nntp 2.86       think 2.12\n",
      "10     com 10.48         know 2.46        guns 2.11\n",
      "Topic 11:\n",
      "             LSA                LDA               NNMF\n",
      "11     com 64.06           edu 5.83           com 8.14\n",
      "11     sun 25.52         lines 4.39       stratus 4.98\n",
      "11      ca 17.34  organization 4.24       article 2.96\n",
      "11     ibm 15.76     university 4.0           sun 2.71\n",
      "11  israel 15.73           com 3.78       posting 2.32\n",
      "11  article 15.1       posting 3.46            sw 2.27\n",
      "11       uk 14.6          host 3.41           ibm 2.23\n",
      "11  stratus 13.7       article 3.33  organization 2.18\n",
      "11     apr 12.77          nntp 3.17          host 2.17\n",
      "11   reply 11.92          like 2.73          nntp 2.17\n",
      "Topic 12:\n",
      "                   LSA                LDA              NNMF\n",
      "12      optilink 18.82           edu 6.69     optilink 4.13\n",
      "12           gay 16.64         lines 4.83       cramer 3.62\n",
      "12        cramer 16.15  organization 4.78          gay 3.57\n",
      "12    homosexual 13.89           com 4.26   homosexual 2.94\n",
      "12           men 12.37    university 4.25          men 2.91\n",
      "12            uk 10.73       article 3.95       people 2.01\n",
      "12           sex 10.54       posting 3.81       sexual 1.86\n",
      "12        article 9.97          host 3.68        study 1.85\n",
      "12  homosexuality 9.96          nntp 3.58  homosexuals 1.78\n",
      "12          study 9.89          know 3.34      clayton 1.68\n",
      "Topic 13:\n",
      "             LSA                 LDA            NNMF\n",
      "13   state 18.34           god 27.28   morality 3.46\n",
      "13    sale 17.61           edu 16.16  objective 2.93\n",
      "13     ohio 13.8         jesus 15.14         sgi 2.9\n",
      "13      usa 12.0        people 14.43       moral 2.6\n",
      "13     god 10.72       article 12.71     people 2.25\n",
      "13  access 10.62           com 11.91         don 2.2\n",
      "13     new 10.43         lines 11.69        edu 2.03\n",
      "13  windows 9.49  organization 11.46        say 2.02\n",
      "13      acs 9.04           don 10.83      think 1.99\n",
      "13      com 8.98          just 10.22     caltech 1.9\n",
      "Topic 14:\n",
      "                LSA                LDA            NNMF\n",
      "14    windows 25.45           edu 7.39    windows 7.59\n",
      "14         com 16.6         lines 5.03        dos 3.68\n",
      "14       cwru 16.08  organization 4.97         ms 1.93\n",
      "14  cleveland 14.25    university 4.95       file 1.89\n",
      "14        dos 13.24       article 4.25         os 1.72\n",
      "14        car 12.06           com 4.17        edu 1.46\n",
      "14          ins 9.4       posting 3.99  microsoft 1.37\n",
      "14     reserve 9.32          host 3.86      mouse 1.31\n",
      "14     western 9.28           nntp 3.8        run 1.19\n",
      "14        bike 9.14           don 3.69        use 1.17\n",
      "Topic 15:\n",
      "              LSA                 LDA               NNMF\n",
      "15     card 31.99           edu 38.62          cwru 4.14\n",
      "15  monitor 21.35         lines 25.05     cleveland 4.07\n",
      "15    video 20.81  organization 24.79       western 2.72\n",
      "15       ibm 11.2    university 24.11            ins 2.7\n",
      "15    apple 11.19          game 24.05       reserve 2.69\n",
      "15       bus 9.93       article 23.57          case 2.17\n",
      "15     cards 9.46           team 20.8           edu 1.93\n",
      "15       vga 9.41       posting 20.36       freenet 1.91\n",
      "15    access 9.22           com 20.21  ohiousalines 1.43\n",
      "15      sale 9.14          host 19.79    university 1.39\n",
      "Topic 16:\n",
      "              LSA                LDA               NNMF\n",
      "16    state 17.77            edu 8.8          card 5.39\n",
      "16     card 17.02         lines 6.83         video 3.36\n",
      "16     ohio 16.07  organization 6.68       monitor 3.16\n",
      "16       ibm 10.9           com 6.59           edu 2.28\n",
      "16      sgi 10.86       article 5.83           mac 2.11\n",
      "16     video 9.97    university 5.71          apple 2.0\n",
      "16    magnus 9.89       posting 5.19           bus 1.89\n",
      "16       acs 9.78          host 4.87         lines 1.85\n",
      "16  morality 9.59          nntp 4.84  organization 1.81\n",
      "16        com 8.6          like 4.74       windows 1.75\n",
      "Topic 17:\n",
      "               LSA                  LDA               NNMF\n",
      "17       edu 25.64           edu 243.96           ohio 6.6\n",
      "17        cs 17.89  organization 172.81          state 5.7\n",
      "17   article 17.84         lines 172.25        magnus 4.12\n",
      "17       apr 17.58           com 165.37           acs 3.84\n",
      "17      pitt 12.12       article 152.54           edu 2.31\n",
      "17       ibm 10.82    university 152.45    university 1.81\n",
      "17      card 10.79       posting 140.48       article 1.18\n",
      "17        cc 10.78           don 136.69       posting 1.12\n",
      "17  computer 10.31          host 134.63  organization 1.12\n",
      "17      uiuc 10.05          just 133.58          host 1.12\n",
      "Topic 18:\n",
      "                 LSA                LDA                 NNMF\n",
      "18           uk 19.9           edu 7.64          access 6.52\n",
      "18           ac 15.4          lines 5.6           digex 3.83\n",
      "18        ohio 12.23  organization 5.57             net 2.73\n",
      "18       state 12.17           com 5.43             usa 2.41\n",
      "18  newsreader 11.63    university 4.85  communications 2.23\n",
      "18     version 11.26        article 4.6          online 2.22\n",
      "18         tin 11.05       posting 4.52         express 2.16\n",
      "18         ibm 10.49          host 4.41    organization 1.59\n",
      "18          pl 10.34          nntp 4.31           lines 1.58\n",
      "18         gun 10.27          know 4.02         posting 1.54\n",
      "Topic 19:\n",
      "              LSA                LDA             NNMF\n",
      "19    apple 18.93           edu 6.45        pitt 4.42\n",
      "19    netcom 13.8            ax 5.46         geb 3.77\n",
      "19       cs 13.79         lines 4.89          cs 3.25\n",
      "19      com 12.39  organization 4.82     science 2.71\n",
      "19      pitt 11.2    university 4.27    computer 2.59\n",
      "19       mac 9.87           com 4.04         edu 2.56\n",
      "19    sandvik 9.3       posting 3.91  pittsburgh 2.31\n",
      "19  computer 9.09        article 3.9   edugordon 2.02\n",
      "19      ohio 9.05          host 3.66       cadre 1.86\n",
      "19    access 8.88          nntp 3.56        njxp 1.84\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
